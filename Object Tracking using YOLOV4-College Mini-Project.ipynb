{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights of convolution #0\n",
      "loading weights of convolution #1\n",
      "loading weights of convolution #2\n",
      "loading weights of convolution #3\n",
      "no convolution #4\n",
      "loading weights of convolution #5\n",
      "loading weights of convolution #6\n",
      "loading weights of convolution #7\n",
      "no convolution #8\n",
      "loading weights of convolution #9\n",
      "loading weights of convolution #10\n",
      "no convolution #11\n",
      "loading weights of convolution #12\n",
      "loading weights of convolution #13\n",
      "loading weights of convolution #14\n",
      "no convolution #15\n",
      "loading weights of convolution #16\n",
      "loading weights of convolution #17\n",
      "no convolution #18\n",
      "loading weights of convolution #19\n",
      "loading weights of convolution #20\n",
      "no convolution #21\n",
      "loading weights of convolution #22\n",
      "loading weights of convolution #23\n",
      "no convolution #24\n",
      "loading weights of convolution #25\n",
      "loading weights of convolution #26\n",
      "no convolution #27\n",
      "loading weights of convolution #28\n",
      "loading weights of convolution #29\n",
      "no convolution #30\n",
      "loading weights of convolution #31\n",
      "loading weights of convolution #32\n",
      "no convolution #33\n",
      "loading weights of convolution #34\n",
      "loading weights of convolution #35\n",
      "no convolution #36\n",
      "loading weights of convolution #37\n",
      "loading weights of convolution #38\n",
      "loading weights of convolution #39\n",
      "no convolution #40\n",
      "loading weights of convolution #41\n",
      "loading weights of convolution #42\n",
      "no convolution #43\n",
      "loading weights of convolution #44\n",
      "loading weights of convolution #45\n",
      "no convolution #46\n",
      "loading weights of convolution #47\n",
      "loading weights of convolution #48\n",
      "no convolution #49\n",
      "loading weights of convolution #50\n",
      "loading weights of convolution #51\n",
      "no convolution #52\n",
      "loading weights of convolution #53\n",
      "loading weights of convolution #54\n",
      "no convolution #55\n",
      "loading weights of convolution #56\n",
      "loading weights of convolution #57\n",
      "no convolution #58\n",
      "loading weights of convolution #59\n",
      "loading weights of convolution #60\n",
      "no convolution #61\n",
      "loading weights of convolution #62\n",
      "loading weights of convolution #63\n",
      "loading weights of convolution #64\n",
      "no convolution #65\n",
      "loading weights of convolution #66\n",
      "loading weights of convolution #67\n",
      "no convolution #68\n",
      "loading weights of convolution #69\n",
      "loading weights of convolution #70\n",
      "no convolution #71\n",
      "loading weights of convolution #72\n",
      "loading weights of convolution #73\n",
      "no convolution #74\n",
      "loading weights of convolution #75\n",
      "loading weights of convolution #76\n",
      "loading weights of convolution #77\n",
      "loading weights of convolution #78\n",
      "loading weights of convolution #79\n",
      "loading weights of convolution #80\n",
      "loading weights of convolution #81\n",
      "no convolution #82\n",
      "no convolution #83\n",
      "loading weights of convolution #84\n",
      "no convolution #85\n",
      "no convolution #86\n",
      "loading weights of convolution #87\n",
      "loading weights of convolution #88\n",
      "loading weights of convolution #89\n",
      "loading weights of convolution #90\n",
      "loading weights of convolution #91\n",
      "loading weights of convolution #92\n",
      "loading weights of convolution #93\n",
      "no convolution #94\n",
      "no convolution #95\n",
      "loading weights of convolution #96\n",
      "no convolution #97\n",
      "no convolution #98\n",
      "loading weights of convolution #99\n",
      "loading weights of convolution #100\n",
      "loading weights of convolution #101\n",
      "loading weights of convolution #102\n",
      "loading weights of convolution #103\n",
      "loading weights of convolution #104\n",
      "loading weights of convolution #105\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hard Parikh\\anaconda3\\envs\\opencv_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "import core.utils as utils\n",
    "from core.yolov4 import filter_boxes\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from core.config import cfg\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "# deep sort imports\n",
    "from deep_sort import preprocessing, nn_matching\n",
    "from deep_sort.detection import Detection\n",
    "from deep_sort.tracker import Tracker\n",
    "from tools import generate_detections as gdet\n",
    "import struct\n",
    "import numpy as np\n",
    "import io\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import add, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from numpy import expand_dims\n",
    "import tensorflow as tf\n",
    "from numpy import expand_dims\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from skimage.segmentation import clear_border\n",
    "import pytesseract\n",
    "import imutils\n",
    "\n",
    "def build_tesseract_options(psm=7):\n",
    "    alphanumeric = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
    "    options = \"-c tessedit_char_whitelist={}\".format(alphanumeric)\n",
    "    options += \" --psm {}\".format(psm)\n",
    "    return options\n",
    " \n",
    "def _conv_block(inp, convs, skip=True):\n",
    "    x = inp\n",
    "    count = 0\n",
    "    for conv in convs:\n",
    "        if count == (len(convs) - 2) and skip:\n",
    "            skip_connection = x\n",
    "        count += 1\n",
    "        if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n",
    "        x = Conv2D(conv['filter'],\n",
    "                   conv['kernel'],\n",
    "                   strides=conv['stride'],\n",
    "                   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n",
    "                   name='conv_' + str(conv['layer_idx']),\n",
    "                   use_bias=False if conv['bnorm'] else True)(x)\n",
    "        if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n",
    "        if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n",
    "    return add([skip_connection, x]) if skip else x\n",
    " \n",
    "def make_yolov3_model():\n",
    "    input_image = Input(shape=(None, None, 3))\n",
    "    # Layer  0 => 4\n",
    "    x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n",
    "                                  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n",
    "                                  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n",
    "    # Layer  5 => 8\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n",
    "                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n",
    "    # Layer  9 => 11\n",
    "    x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n",
    "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n",
    "    # Layer 12 => 15\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n",
    "                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n",
    "                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n",
    "    # Layer 16 => 36\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n",
    "                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n",
    "    skip_36 = x\n",
    "    # Layer 37 => 40\n",
    "    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n",
    "    # Layer 41 => 61\n",
    "    for i in range(7):\n",
    "        x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n",
    "                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n",
    "    skip_61 = x\n",
    "    # Layer 62 => 65\n",
    "    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n",
    "    # Layer 66 => 74\n",
    "    for i in range(3):\n",
    "        x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n",
    "                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n",
    "    # Layer 75 => 79\n",
    "    x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n",
    "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n",
    "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n",
    "    # Layer 80 => 82\n",
    "    yolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n",
    "                              {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n",
    "    # Layer 83 => 86\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n",
    "    x = UpSampling2D(2)(x)\n",
    "    x = concatenate([x, skip_61])\n",
    "    # Layer 87 => 91\n",
    "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n",
    "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n",
    "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n",
    "    # Layer 92 => 94\n",
    "    yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n",
    "                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n",
    "    # Layer 95 => 98\n",
    "    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n",
    "    x = UpSampling2D(2)(x)\n",
    "    x = concatenate([x, skip_36])\n",
    "    # Layer 99 => 106\n",
    "    yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n",
    "                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n",
    "                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n",
    "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n",
    "                               {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n",
    "    model = Model(input_image, [yolo_82, yolo_94, yolo_106])\n",
    "    return model\n",
    " \n",
    "class WeightReader:\n",
    "    def __init__(self, weight_file):\n",
    "        with open(weight_file, 'rb') as w_f:\n",
    "            major,\t= struct.unpack('i', w_f.read(4))\n",
    "            minor,\t= struct.unpack('i', w_f.read(4))\n",
    "            revision, = struct.unpack('i', w_f.read(4))\n",
    "            if (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n",
    "                w_f.read(8)\n",
    "            else:\n",
    "                w_f.read(4)\n",
    "            transpose = (major > 1000) or (minor > 1000)\n",
    "            binary = w_f.read()\n",
    "        self.offset = 0\n",
    "        self.all_weights = np.frombuffer(binary, dtype='float32')\n",
    " \n",
    "    def read_bytes(self, size):\n",
    "        self.offset = self.offset + size\n",
    "        return self.all_weights[self.offset-size:self.offset]\n",
    " \n",
    "    def load_weights(self, model):\n",
    "        for i in range(106):\n",
    "            try:\n",
    "                conv_layer = model.get_layer('conv_' + str(i))\n",
    "                print(\"loading weights of convolution #\" + str(i))\n",
    "                if i not in [81, 93, 105]:\n",
    "                    norm_layer = model.get_layer('bnorm_' + str(i))\n",
    "                    size = np.prod(norm_layer.get_weights()[0].shape)\n",
    "                    beta  = self.read_bytes(size) # bias\n",
    "                    gamma = self.read_bytes(size) # scale\n",
    "                    mean  = self.read_bytes(size) # mean\n",
    "                    var   = self.read_bytes(size) # variance\n",
    "                    weights = norm_layer.set_weights([gamma, beta, mean, var])\n",
    "                if len(conv_layer.get_weights()) > 1:\n",
    "                    bias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
    "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "                    kernel = kernel.transpose([2,3,1,0])\n",
    "                    conv_layer.set_weights([kernel, bias])\n",
    "                else:\n",
    "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
    "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "                    kernel = kernel.transpose([2,3,1,0])\n",
    "                    conv_layer.set_weights([kernel])\n",
    "            except ValueError:\n",
    "                print(\"no convolution #\" + str(i))\n",
    " \n",
    "    def reset(self):\n",
    "        self.offset = 0\n",
    " \n",
    "# define the model\n",
    "model = make_yolov3_model()\n",
    "# load the model weights\n",
    "weight_reader = WeightReader(r\"C:\\Users\\Hard Parikh\\Desktop\\yolov3.weights\")\n",
    "# set the model weights into the model\n",
    "weight_reader.load_weights(model)\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# load and prepare an image\n",
    "def load_image_pixels(filename, shape):\n",
    "    # load the image to get its shape\n",
    "    image = load_img(filename)\n",
    "    width, height = image.size\n",
    "    # load the image with the required size\n",
    "    image = load_img(filename, target_size=shape)\n",
    "    # convert to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # scale pixel values to [0, 1]\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0\n",
    "    # add a dimension so that we have one sample\n",
    "    image = expand_dims(image, 0)\n",
    "    return image, width, height\n",
    " \n",
    "# load yolov3 model\n",
    "# model = load_model('model.h5')\n",
    "# define the expected input shape for the model\n",
    "# input_w, input_h = 416, 416\n",
    "# define our new photo\n",
    "# photo_filename = r\"C:\\Users\\Hard Parikh\\darknet-car\\darknet-master\\build\\darknet\\x64\\dog.jpg\"\n",
    "# load and prepare image\n",
    "# image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))\n",
    "# make prediction\n",
    "# yhat = model.predict(image)\n",
    "# summarize the shape of the list of arrays\n",
    "# print([a.shape for a in yhat])\n",
    "\n",
    "\n",
    " \n",
    "class BoundBox:\n",
    "    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        self.objness = objness\n",
    "        self.classes = classes\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    " \n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)\n",
    " \n",
    "        return self.label\n",
    " \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    " \n",
    "        return self.score\n",
    " \n",
    "def _sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    " \n",
    "def decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n",
    "    grid_h, grid_w = netout.shape[:2]\n",
    "    nb_box = 3\n",
    "    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
    "    nb_class = netout.shape[-1] - 5\n",
    "    boxes = []\n",
    "    netout[..., :2]  = _sigmoid(netout[..., :2])\n",
    "    netout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
    "    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
    "    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
    " \n",
    "    for i in range(grid_h*grid_w):\n",
    "        row = i / grid_w\n",
    "        col = i % grid_w\n",
    "        for b in range(nb_box):\n",
    "            # 4th element is objectness score\n",
    "            objectness = netout[int(row)][int(col)][b][4]\n",
    "            if(objectness.all() <= obj_thresh): continue\n",
    "            # first 4 elements are x, y, w, and h\n",
    "            x, y, w, h = netout[int(row)][int(col)][b][:4]\n",
    "            x = (col + x) / grid_w # center position, unit: image width\n",
    "            y = (row + y) / grid_h # center position, unit: image height\n",
    "            w = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
    "            h = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n",
    "            # last elements are class probabilities\n",
    "            classes = netout[int(row)][col][b][5:]\n",
    "            box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
    "            boxes.append(box)\n",
    "    return boxes\n",
    " \n",
    "def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n",
    "    new_w, new_h = net_w, net_h\n",
    "    for i in range(len(boxes)):\n",
    "        x_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
    "        y_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
    "        boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
    "        boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
    "        boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
    "        boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n",
    " \n",
    "def _interval_overlap(interval_a, interval_b):\n",
    "    x1, x2 = interval_a\n",
    "    x3, x4 = interval_b\n",
    "    if x3 < x1:\n",
    "        if x4 < x1:\n",
    "            return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x1\n",
    "    else:\n",
    "        if x2 < x3:\n",
    "             return 0\n",
    "        else:\n",
    "            return min(x2,x4) - x3\n",
    " \n",
    "def bbox_iou(box1, box2):\n",
    "    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
    "    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
    "    intersect = intersect_w * intersect_h\n",
    "    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
    "    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
    "    union = w1*h1 + w2*h2 - intersect\n",
    "    return float(intersect) / union\n",
    " \n",
    "def do_nms(boxes, nms_thresh):\n",
    "    if len(boxes) > 0:\n",
    "        nb_class = len(boxes[0].classes)\n",
    "    else:\n",
    "        return\n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "            if boxes[index_i].classes[c] == 0: continue\n",
    "            for j in range(i+1, len(sorted_indices)):\n",
    "                index_j = sorted_indices[j]\n",
    "                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
    "                    boxes[index_j].classes[c] = 0\n",
    " \n",
    "# load and prepare an image\n",
    "def load_image_pixels(filename, shape):\n",
    "    # load the image to get its shape\n",
    "    image = load_img(filename)\n",
    "    width, height = image.size\n",
    "    # load the image with the required size\n",
    "    image = load_img(filename, target_size=shape)\n",
    "    # convert to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # scale pixel values to [0, 1]\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0\n",
    "    # add a dimension so that we have one sample\n",
    "    image = expand_dims(image, 0)\n",
    "    return image, width, height\n",
    "\n",
    "def load_video_pixels(image, shape):\n",
    "    # load the image to get its shape\n",
    "    width, height = image.shape[1],image.shape[0]\n",
    "    # load the image with the required size\n",
    "    image=cv2.resize(image,shape)\n",
    "#     image = load_img(filename, target_size=shape)\n",
    "    # convert to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # scale pixel values to [0, 1]\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0\n",
    "    # add a dimension so that we have one sample\n",
    "    image = expand_dims(image, 0)\n",
    "    return image, width, height\n",
    " \n",
    "# get all of the results above a threshold\n",
    "def get_boxes(boxes, labels, thresh):\n",
    "    v_boxes, v_labels, v_scores = list(), list(), list()\n",
    "    # enumerate all boxes\n",
    "    for box in boxes:\n",
    "        # enumerate all possible labels\n",
    "        for i in range(len(labels)):\n",
    "            # check if the threshold for this label is high enough\n",
    "            if box.classes[i] > thresh:\n",
    "                v_boxes.append(box)\n",
    "                v_labels.append(labels[i])\n",
    "                v_scores.append(box.classes[i]*100)\n",
    "                # don't break, many labels may trigger for one box\n",
    "    return v_boxes, v_labels, v_scores\n",
    " \n",
    "# draw all results\n",
    "def draw_boxes(filename, v_boxes, v_labels, v_scores):\n",
    "    # load the image\n",
    "    data = pyplot.imread(filename)\n",
    "    # plot the image\n",
    "    pyplot.imshow(data)\n",
    "    # get the context for drawing boxes\n",
    "    ax = pyplot.gca()\n",
    "    # plot each box\n",
    "    crop_cars=[]\n",
    "    for i in range(len(v_boxes)):\n",
    "        if(v_labels[i]=='car'):\n",
    "            box = v_boxes[i]\n",
    "            # get coordinates\n",
    "            y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
    "            # calculate width and height of the box\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            # create the shape\n",
    "            crop_cars.append(data[int(0.7*y1):int(1.4*y2),x1:int(1.1*x2)])\n",
    "#             crop_cars.append(data[y1:y2,x1:x2])\n",
    "            rect = Rectangle((x1, y1), width, height, fill=False, color='white')\n",
    "            # draw the box\n",
    "            ax.add_patch(rect)\n",
    "            # draw text and score in top left corner\n",
    "            label = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n",
    "            pyplot.text(x1, y1, label, color='white')\n",
    "        else:\n",
    "            pass\n",
    "    pyplot.show()\n",
    "    return crop_cars\n",
    "\n",
    "# load yolov3 model\n",
    "model = load_model('model.h5')\n",
    "# define the expected input shape for the model\n",
    "input_w, input_h = 416, 416\n",
    "# define our new photo\n",
    "\n",
    "LABELS_FILE=r\"C:\\Users\\Hard Parikh\\darknet-car_type\\darknet-master\\build\\darknet\\x64\\data\\obj.names.names\"\n",
    "CONFIG_FILE=r\"C:\\Users\\Hard Parikh\\darknet-car_type\\darknet-master\\build\\darknet\\x64\\cfg\\yolov4-obj.cfg\"\n",
    "WEIGHTS_FILE=r\"C:\\Users\\Hard Parikh\\darknet-car_type\\darknet-master\\build\\darknet\\x64\\backup\\yolov4-obj_final.weights\"\n",
    "CONFIDENCE_THRESHOLD=0.5\n",
    "LABELS_FILE_P=r\"C:\\Users\\Hard Parikh\\darknet-master\\darknet-master\\build\\darknet\\x64\\data\\obj.names.names\"\n",
    "CONFIG_FILE_P=r\"C:\\Users\\Hard Parikh\\darknet-master\\darknet-master\\build\\darknet\\x64\\cfg\\yolov4-obj.cfg\"\n",
    "WEIGHTS_FILE_P=r\"C:\\Users\\Hard Parikh\\darknet-master\\darknet-master\\build\\darknet\\x64\\backup\\yolov4-obj_final.weights\"\n",
    "CONFIDENCE_THRESHOLD_P=0.5\n",
    "LABELS = open(LABELS_FILE).read().strip().split(\"\\n\")\n",
    "LABELS_P = open(LABELS_FILE_P).read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_coordinates(image):\n",
    "    photo_filename = image\n",
    "    car_images=[photo_filename]\n",
    "    image, image_w, image_h = load_video_pixels(photo_filename, (input_w, input_h))\n",
    "\n",
    "    net = cv2.dnn.readNetFromDarknet(CONFIG_FILE, WEIGHTS_FILE)\n",
    "    net_P = cv2.dnn.readNetFromDarknet(CONFIG_FILE_P, WEIGHTS_FILE_P)\n",
    "    # license_plates=[]\n",
    "    if(len(car_images)!=0):\n",
    "        car_type_cor=[]\n",
    "        license_plate_cor=[]\n",
    "        for i in range(len(car_images)):\n",
    "            image = car_images[i]\n",
    "            (H, W) = image.shape[:2]\n",
    "\n",
    "            ln = net.getLayerNames()\n",
    "            ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "            ln_P = net_P.getLayerNames()\n",
    "            ln_P = [ln_P[i[0] - 1] for i in net_P.getUnconnectedOutLayers()]\n",
    "\n",
    "\n",
    "            blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
    "                swapRB=True, crop=False)\n",
    "            net.setInput(blob)\n",
    "            layerOutputs = net.forward(ln)  \n",
    "\n",
    "            blob_P = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
    "                swapRB=True, crop=False)\n",
    "            net_P.setInput(blob_P)\n",
    "            layerOutputsP = net_P.forward(ln_P)\n",
    "            boxes = []\n",
    "            confidences = []\n",
    "            classIDs = []\n",
    "\n",
    "            for output in layerOutputs:\n",
    "                for detection in output:\n",
    "                    scores = detection[5:]\n",
    "                    classID = np.argmax(scores)\n",
    "                    confidence = scores[classID]\n",
    "\n",
    "                    if confidence > CONFIDENCE_THRESHOLD:\n",
    "                        box = detection[0:4] * np.array([W, H, W, H])\n",
    "                        (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                        x = int(centerX - (width / 2))\n",
    "                        y = int(centerY - (height / 2))\n",
    "\n",
    "                        boxes.append([x, y, int(width), int(height)])\n",
    "                        confidences.append(float(confidence))\n",
    "                        classIDs.append(classID)\n",
    "\n",
    "            idxs = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD,CONFIDENCE_THRESHOLD)\n",
    "            if len(idxs) > 0:\n",
    "                for j in idxs.flatten():\n",
    "                    (x, y) = (boxes[j][0], boxes[j][1])\n",
    "                    (w, h) = (boxes[j][2], boxes[j][3])\n",
    "                    car_type_cor.append([x,x+w,y,y+h])\n",
    "                    try:\n",
    "                        text = \"{}: {:.4f}\".format(LABELS[classIDs[j]], confidences[classIDs[j]])\n",
    "                    except:\n",
    "                        text=''\n",
    "            else:\n",
    "                text=''\n",
    "                car_type_cor.append(None)\n",
    "            boxesP = []\n",
    "            confidencesP = []\n",
    "            classIDsP = []\n",
    "\n",
    "            for output in layerOutputsP:\n",
    "                for detection in output:\n",
    "                    scores = detection[5:]\n",
    "                    classID = np.argmax(scores)\n",
    "                    confidence = scores[classID]\n",
    "\n",
    "                    if confidence > CONFIDENCE_THRESHOLD_P:\n",
    "                        box = detection[0:4] * np.array([W, H, W, H])\n",
    "                        (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                        x = int(centerX - (width / 2))\n",
    "                        y = int(centerY - (height / 2))\n",
    "\n",
    "                        boxesP.append([x, y, int(width), int(height)])\n",
    "                        confidencesP.append(float(confidence))\n",
    "                        classIDsP.append(classID)\n",
    "            textP='License Plate'\n",
    "            idxsP = cv2.dnn.NMSBoxes(boxesP, confidencesP, CONFIDENCE_THRESHOLD_P,CONFIDENCE_THRESHOLD_P)\n",
    "            try:\n",
    "                license_plates=[]\n",
    "                if len(idxsP) > 0:\n",
    "                    for j in idxsP.flatten():\n",
    "                        (x, y) = (boxesP[j][0], boxesP[j][1])\n",
    "                        (w, h) = (boxesP[j][2], boxesP[j][3])\n",
    "                        license_plate_image=image[y:y+h,x:x+w]\n",
    "                        license_plates.append(license_plate_image)\n",
    "                        gray=cv2.cvtColor(license_plate_image,cv2.COLOR_BGR2GRAY)\n",
    "                        options=build_tesseract_options()\n",
    "                        try:\n",
    "                            lpText=pytesseract.image_to_string(gray,config=options)\n",
    "                            cv2.rectangle(image, (x, y), (x + w, y + h), (255,255,255), 2)\n",
    "                            textP=lpText.split('\\n')[0]\n",
    "                            license_plate_cor.append([x,x+w,y,y+h])\n",
    "                            license_plates.append(image[y:y+h,x:x+w])\n",
    "#                             textP = \"{}: {:.4f}\".format(LABELS_P[classIDsP[j]], confidencesP[classIDsP[j]])\n",
    "                        except:\n",
    "                            textP='License Plate'\n",
    "#                         cv2.putText(image, textP, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,1.75, (255,255,255), 3)\n",
    "                else:\n",
    "                    textP=''\n",
    "                    license_plate_cor.append(None)\n",
    "            except:\n",
    "                pass\n",
    "    return car_type_cor,license_plate_cor,text,textP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path=r\"C:\\Users\\Hard Parikh\\yolov4-deepsort-master\\yolov4-deepsort-master\\data\\video\\cars.mp4\"\n",
    "output=r\"C:\\Users\\Hard Parikh\\yolov4-deepsort-master\\yolov4-deepsort-master\\outputs\\demo_notebook.avi\"\n",
    "CLASSES=r\"C:\\Users\\Hard Parikh\\yolov4-deepsort-master\\yolov4-deepsort-master\\data\\classes\\coco.names\"\n",
    "allowed_classes=['car']\n",
    "dont_show=False\n",
    "def video_tracking(video_path,output,CLASSES,allowed_classes,dont_show):\n",
    "    frames=[]\n",
    "    max_cosine_distance = 0.4\n",
    "    nn_budget = None\n",
    "    nms_max_overlap = 1.0\n",
    "\n",
    "    model_filename = r\"C:\\Users\\Hard Parikh\\yolov4-deepsort-master\\yolov4-deepsort-master\\model_data\\mars-small128.pb\"\n",
    "    encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
    "    metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "    tracker = Tracker(metric)\n",
    "\n",
    "    config = ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = InteractiveSession(config=config)\n",
    "    input_size = 416\n",
    "    video_path = video_path\n",
    "\n",
    "    saved_model_loaded = tf.saved_model.load(r\"C:\\Users\\Hard Parikh\\yolov4-deepsort-master\\yolov4-deepsort-master\\checkpoints\\yolov4-416\", tags=[tag_constants.SERVING])\n",
    "    infer = saved_model_loaded.signatures['serving_default']\n",
    "\n",
    "    try:\n",
    "        vid = cv2.VideoCapture(int(video_path))\n",
    "    except:\n",
    "        vid = cv2.VideoCapture(video_path)\n",
    "\n",
    "    out = None\n",
    "\n",
    "    output=output\n",
    "    if output:\n",
    "        width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "        codec = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(output, codec, fps, (width, height))\n",
    "\n",
    "    frame_num = 0\n",
    "    while True:\n",
    "        return_value, frame = vid.read()\n",
    "        if return_value:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(frame)\n",
    "        else:\n",
    "            print('Video has ended or failed, try a different video format!')\n",
    "            break\n",
    "        frame_num +=1\n",
    "        print('Frame #: ', frame_num)\n",
    "        frame_size = frame.shape[:2]\n",
    "        image_data = cv2.resize(frame, (input_size, input_size))\n",
    "        image_data = image_data / 255.\n",
    "        image_data = image_data[np.newaxis, ...].astype(np.float32)\n",
    "        start_time = time.time()\n",
    "\n",
    "        batch_data = tf.constant(image_data)\n",
    "        pred_bbox = infer(batch_data)\n",
    "        for key, value in pred_bbox.items():\n",
    "            boxes = value[:, :, 0:4]\n",
    "            pred_conf = value[:, :, 4:]\n",
    "\n",
    "        boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(\n",
    "            boxes=tf.reshape(boxes, (tf.shape(boxes)[0], -1, 1, 4)),\n",
    "            scores=tf.reshape(\n",
    "                pred_conf, (tf.shape(pred_conf)[0], -1, tf.shape(pred_conf)[-1])),\n",
    "            max_output_size_per_class=50,\n",
    "            max_total_size=50,\n",
    "            iou_threshold=0.45,\n",
    "            score_threshold=0.5\n",
    "        )\n",
    "\n",
    "        num_objects = valid_detections.numpy()[0]\n",
    "        bboxes = boxes.numpy()[0]\n",
    "        bboxes = bboxes[0:int(num_objects)]\n",
    "        scores = scores.numpy()[0]\n",
    "        scores = scores[0:int(num_objects)]\n",
    "        classes = classes.numpy()[0]\n",
    "        classes = classes[0:int(num_objects)]\n",
    "\n",
    "        original_h, original_w, _ = frame.shape\n",
    "        bboxes = utils.format_boxes(bboxes, original_h, original_w)\n",
    "\n",
    "        pred_bbox = [bboxes, scores, classes, num_objects]\n",
    "\n",
    "\n",
    "        CLASSES=CLASSES\n",
    "        class_names = utils.read_class_names(CLASSES)\n",
    "\n",
    "        #allowed_classes = list(class_names.values())\n",
    "        allowed_classes=allowed_classes\n",
    "\n",
    "        names = []\n",
    "        deleted_indx = []\n",
    "        for i in range(num_objects):\n",
    "            class_indx = int(classes[i])\n",
    "            class_name = class_names[class_indx]\n",
    "            if class_name not in allowed_classes:\n",
    "                deleted_indx.append(i)\n",
    "            else:\n",
    "                names.append(class_name)\n",
    "        names = np.array(names)\n",
    "        count = len(names)\n",
    "        count=True\n",
    "        dont_show=dont_show\n",
    "        if count:\n",
    "            cv2.putText(frame, \"Objects being tracked: {}\".format(count), (5, 35), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (0, 255, 0), 2)\n",
    "            print(\"Objects being tracked: {}\".format(count))\n",
    "        bboxes = np.delete(bboxes, deleted_indx, axis=0)\n",
    "        scores = np.delete(scores, deleted_indx, axis=0)\n",
    "\n",
    "        features = encoder(frame, bboxes)\n",
    "        detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in zip(bboxes, scores, names, features)]\n",
    "\n",
    "        cmap = plt.get_cmap('tab20b')\n",
    "        colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "        boxs = np.array([d.tlwh for d in detections])\n",
    "        scores = np.array([d.confidence for d in detections])\n",
    "        classes = np.array([d.class_name for d in detections])\n",
    "        indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores)\n",
    "        detections = [detections[i] for i in indices]       \n",
    "\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "        info=True\n",
    "        for track in tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue \n",
    "            bbox = track.to_tlbr()\n",
    "            class_name = track.get_class()\n",
    "            \n",
    "            result = np.asarray(frame)\n",
    "            result = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            car_image_temp=result[int(bbox[1]):int(bbox[3]),int(bbox[0]):int(bbox[2])]\n",
    "            frames.append([result,[int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])]])\n",
    "#             car_image_temp=frames[i][0][frames[i][1][1]:frames[i][1][3],frames[i][1][0]:frames[i][1][2]]\n",
    "            x_,y_,z_,w_=find_coordinates(car_image_temp)\n",
    "            \n",
    "            color = colors[int(track.track_id) % len(colors)]\n",
    "            color = [i * 255 for i in color]\n",
    "            cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n",
    "            cv2.rectangle(frame, (int(bbox[0]), int(bbox[1]-30)), (int(bbox[0])+(len(class_name)+len(str(track.track_id)))*17, int(bbox[1])), color, -1)\n",
    "            cv2.putText(frame, class_name + \"-\" + str(track.track_id)+\"-\"+str(z_)+\"-\"+str(w_),(int(bbox[0]), int(bbox[1]-10)),0, 0.75, (255,255,255),2)\n",
    "\n",
    "#             if info:\n",
    "#                 print(\"Tracker ID: {}, Class: {},  BBox Coords (xmin, ymin, xmax, ymax): {}\".format(str(track.track_id), class_name, (int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3]))))\n",
    "\n",
    "        fps = 1.0 / (time.time() - start_time)\n",
    "        print(\"FPS: %.2f\" % fps)\n",
    "        result = np.asarray(frame)\n",
    "        result = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        if not dont_show:\n",
    "            cv2.imshow(\"Output Video\", result)\n",
    "        \n",
    "        if output:\n",
    "            out.write(result)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "    cv2.destroyAllWindows()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame #:  1\n",
      "Objects being tracked: True\n",
      "FPS: 0.67\n",
      "Frame #:  2\n",
      "Objects being tracked: True\n",
      "FPS: 1.84\n",
      "Frame #:  3\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  4\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  5\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  6\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  7\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  8\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  9\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  10\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  11\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  12\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  13\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  14\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  15\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  16\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  17\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  18\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  19\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  20\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  21\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  22\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  23\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  24\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  25\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  26\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  27\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  28\n",
      "Objects being tracked: True\n",
      "FPS: 0.05\n",
      "Frame #:  29\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  30\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  31\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  32\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  33\n",
      "Objects being tracked: True\n",
      "FPS: 0.05\n",
      "Frame #:  34\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  35\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  36\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  37\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  38\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  39\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  40\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  41\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  42\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  43\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  44\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  45\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  46\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  47\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  48\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  49\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  50\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  51\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  52\n",
      "Objects being tracked: True\n",
      "FPS: 0.05\n",
      "Frame #:  53\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  54\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  55\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  56\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  57\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  58\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  59\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  60\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  61\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  62\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  63\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  64\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  65\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  66\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  67\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  68\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  69\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  70\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  71\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  72\n",
      "Objects being tracked: True\n",
      "FPS: 0.06\n",
      "Frame #:  73\n",
      "Objects being tracked: True\n",
      "FPS: 0.08\n",
      "Frame #:  74\n",
      "Objects being tracked: True\n",
      "FPS: 0.08\n",
      "Frame #:  75\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  76\n",
      "Objects being tracked: True\n",
      "FPS: 0.08\n",
      "Frame #:  77\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  78\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  79\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  80\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  81\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  82\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  83\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  84\n",
      "Objects being tracked: True\n",
      "FPS: 0.08\n",
      "Frame #:  85\n",
      "Objects being tracked: True\n",
      "FPS: 0.08\n",
      "Frame #:  86\n",
      "Objects being tracked: True\n",
      "FPS: 0.08\n",
      "Frame #:  87\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  88\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  89\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  90\n",
      "Objects being tracked: True\n",
      "FPS: 0.08\n",
      "Frame #:  91\n",
      "Objects being tracked: True\n",
      "FPS: 0.08\n",
      "Frame #:  92\n",
      "Objects being tracked: True\n",
      "FPS: 0.08\n",
      "Frame #:  93\n",
      "Objects being tracked: True\n",
      "FPS: 0.08\n",
      "Frame #:  94\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  95\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  96\n",
      "Objects being tracked: True\n",
      "FPS: 0.08\n",
      "Frame #:  97\n",
      "Objects being tracked: True\n",
      "FPS: 0.08\n",
      "Frame #:  98\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  99\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  100\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  101\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  102\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  103\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  104\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  105\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  106\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  107\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  108\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  109\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  110\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  111\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  112\n",
      "Objects being tracked: True\n",
      "FPS: 0.08\n",
      "Frame #:  113\n",
      "Objects being tracked: True\n",
      "FPS: 0.07\n",
      "Frame #:  114\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  115\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  116\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  117\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  118\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  119\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  120\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  121\n",
      "Objects being tracked: True\n",
      "FPS: 0.11\n",
      "Frame #:  122\n",
      "Objects being tracked: True\n",
      "FPS: 0.11\n",
      "Frame #:  123\n",
      "Objects being tracked: True\n",
      "FPS: 0.11\n",
      "Frame #:  124\n",
      "Objects being tracked: True\n",
      "FPS: 0.11\n",
      "Frame #:  125\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  126\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  127\n",
      "Objects being tracked: True\n",
      "FPS: 0.11\n",
      "Frame #:  128\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  129\n",
      "Objects being tracked: True\n",
      "FPS: 0.13\n",
      "Frame #:  130\n",
      "Objects being tracked: True\n",
      "FPS: 0.11\n",
      "Frame #:  131\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  132\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  133\n",
      "Objects being tracked: True\n",
      "FPS: 0.13\n",
      "Frame #:  134\n",
      "Objects being tracked: True\n",
      "FPS: 0.11\n",
      "Frame #:  135\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  136\n",
      "Objects being tracked: True\n",
      "FPS: 0.11\n",
      "Frame #:  137\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  138\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  139\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  140\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  141\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  142\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  143\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  144\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  145\n",
      "Objects being tracked: True\n",
      "FPS: 0.13\n",
      "Frame #:  146\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  147\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  148\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  149\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  150\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  151\n",
      "Objects being tracked: True\n",
      "FPS: 0.08\n",
      "Frame #:  152\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  153\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  154\n",
      "Objects being tracked: True\n",
      "FPS: 0.09\n",
      "Frame #:  155\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  156\n",
      "Objects being tracked: True\n",
      "FPS: 0.10\n",
      "Frame #:  157\n",
      "Objects being tracked: True\n",
      "FPS: 0.11\n",
      "Frame #:  158\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  159\n",
      "Objects being tracked: True\n",
      "FPS: 0.11\n",
      "Frame #:  160\n",
      "Objects being tracked: True\n",
      "FPS: 0.11\n",
      "Frame #:  161\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  162\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  163\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  164\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  165\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  166\n",
      "Objects being tracked: True\n",
      "FPS: 0.13\n",
      "Frame #:  167\n",
      "Objects being tracked: True\n",
      "FPS: 0.17\n",
      "Frame #:  168\n",
      "Objects being tracked: True\n",
      "FPS: 0.18\n",
      "Frame #:  169\n",
      "Objects being tracked: True\n",
      "FPS: 0.16\n",
      "Frame #:  170\n",
      "Objects being tracked: True\n",
      "FPS: 0.15\n",
      "Frame #:  171\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  172\n",
      "Objects being tracked: True\n",
      "FPS: 0.12\n",
      "Frame #:  173\n",
      "Objects being tracked: True\n",
      "FPS: 0.13\n",
      "Frame #:  174\n",
      "Objects being tracked: True\n",
      "FPS: 0.16\n",
      "Frame #:  175\n",
      "Objects being tracked: True\n",
      "FPS: 0.16\n",
      "Frame #:  176\n",
      "Objects being tracked: True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d0ed6f05481d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mframes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvideo_tracking\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mCLASSES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mallowed_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-eb9dc3b58176>\u001b[0m in \u001b[0;36mvideo_tracking\u001b[1;34m(video_path, output, CLASSES, allowed_classes, dont_show)\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;31m#             car_image_temp=frames[i][0][frames[i][1][1]:frames[i][1][3],frames[i][1][0]:frames[i][1][2]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mx_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_coordinates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcar_image_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_id\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-cbbd17bce767>\u001b[0m in \u001b[0;36mfind_coordinates\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     29\u001b[0m                 swapRB=True, crop=False)\n\u001b[0;32m     30\u001b[0m             \u001b[0mnet_P\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob_P\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mlayerOutputsP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet_P\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mln_P\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mconfidences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frames=video_tracking(video_path,output,CLASSES,allowed_classes,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
